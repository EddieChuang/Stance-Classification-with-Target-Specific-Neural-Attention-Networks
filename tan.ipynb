{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "# utils\n",
    "def read_train_data(train_file):\n",
    "    train_df = pd.read_csv(train_file, usecols=[3, 4, 7], keep_default_na=False)\n",
    "    train_df = train_df.drop(train_df[(train_df['title1_zh'] == '') | (train_df['title2_zh'] == '')].index)\n",
    "    \n",
    "    train_df['title1_zh'] = train_df['title1_zh'].map(segment_word)\n",
    "    train_df['title2_zh'] = train_df['title2_zh'].map(segment_word)\n",
    "    train_df['label'] = train_df['label'].map({'agreed': 0, 'disagreed': 1, 'unrelated': 2})\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "\n",
    "def read_test_data(test_file):\n",
    "    test_df = pd.read_csv(test_file, usecols=[0, 3, 4], keep_default_na=False)\n",
    "    \n",
    "    test_df['title1_zh'] = test_df['title1_zh'].map(segment_word)\n",
    "    test_df['title2_zh'] = test_df['title2_zh'].map(segment_word)\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "    \n",
    "def segment_word(sentence):\n",
    "    words = list(jieba.cut(sentence.strip()))\n",
    "    return remove_punctuation(words)\n",
    "\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    def remove_emptystring(words):\n",
    "        return [w for w in words if w]\n",
    "    \n",
    "    return remove_emptystring(re.sub(r'[^\\w]', '', w) for w in words)\n",
    "\n",
    "\n",
    "def load_wordvector(wordvec_file, UNK, PAD):\n",
    "    wordvector, vocab = [], []\n",
    "    word2index = {}\n",
    "    with open(wordvec_file, 'r', encoding='utf-8') as file:\n",
    "        next(file)  # skip header\n",
    "        for i, row in enumerate(file):\n",
    "            row = row.strip().split(' ')\n",
    "            wordvector.append(row[1:])\n",
    "            vocab.append(row[0])\n",
    "            word2index[row[0]] = i\n",
    "    \n",
    "    wordvector.extend([[0] * len(wordvector[0])] * 2)  # <UNK>, <PAD>\n",
    "    vocab.extend([UNK, PAD])\n",
    "    word2index.update({UNK: len(wordvector) - 2, PAD: len(wordvector) - 1})\n",
    "    return np.array(wordvector).astype(float), word2index, vocab\n",
    "\n",
    "    \n",
    "def process_unknown(sentA, sentB, vocab, UNK):\n",
    "    new_sentA, new_sentB = [], []\n",
    "    for A, B in zip(sentA, sentB):\n",
    "        new_sentA.append([word if word in vocab else UNK for word in A])\n",
    "        new_sentB.append([word if word in vocab else UNK for word in B])\n",
    "    return new_sentA, new_sentB\n",
    "\n",
    "\n",
    "def word_to_index(sentA, sentB, word2index):\n",
    "    new_sentA, new_sentB = [], []\n",
    "    for A, B in zip(sentA, sentB):\n",
    "        new_sentA.append([word2index[word] for word in A])\n",
    "        new_sentB.append([word2index[word] for word in B])\n",
    "    return new_sentA, new_sentB\n",
    "\n",
    "\n",
    "def train_val_split(sentA, sentB, seq_lenA, seq_lenB, label, train_ratio=0.7):\n",
    "    train_len  = int(len(sentA) * train_ratio)\n",
    "    train_data = [sentA[: train_len], sentB[: train_len], seq_lenA[: train_len], seq_lenB[: train_len], label[: train_len]]\n",
    "    val_data   = [sentA[train_len: ], sentB[train_len: ], seq_lenA[train_len: ], seq_lenB[train_len: ], label[train_len: ]]\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def shuffle_data(data):\n",
    "    indice = np.arange(len(data[0]))\n",
    "    np.random.shuffle(indice)\n",
    "    \n",
    "    return [data[0][indice], data[1][indice], data[2][indice], data[3][indice], data[4][indice]]\n",
    "\n",
    "\n",
    "def next_batch(data, batch_size, word2index):\n",
    "    def pad(sequence, max_len):\n",
    "        return np.array([seq + [word2index['<PAD>']] * (max_len - len(seq)) for seq in sequence])\n",
    "    \n",
    "    sentA, sentB, seq_lenA, seq_lenB, label = data[0], data[1], data[2], data[3], data[4]\n",
    "    nbatch = len(data[0]) // batch_size\n",
    "    for i in range(nbatch):\n",
    "        offset = i * batch_size\n",
    "        \n",
    "        batch_seq_lenA = seq_lenA[offset: offset + batch_size]\n",
    "        batch_seq_lenB = seq_lenB[offset: offset + batch_size]\n",
    "        \n",
    "        batch_sentA = pad(sentA[offset: offset + batch_size], max(batch_seq_lenA))\n",
    "        batch_sentB = pad(sentB[offset: offset + batch_size], max(batch_seq_lenB))\n",
    "        batch_label = label[offset: offset + batch_size] if label.any() else []\n",
    "        \n",
    "        yield batch_sentA, batch_sentB, batch_seq_lenA, batch_seq_lenB, batch_label\n",
    "    \n",
    "    offset = nbatch * batch_size\n",
    "    \n",
    "    batch_seq_lenA = seq_lenA[offset: offset + batch_size]\n",
    "    batch_seq_lenB = seq_lenB[offset: offset + batch_size]\n",
    "    batch_sentA = pad(sentA[offset: offset + batch_size], max(batch_seq_lenA))\n",
    "    batch_sentB = pad(sentB[offset: offset + batch_size], max(batch_seq_lenB))\n",
    "    batch_label = label[offset: offset + batch_size] if label.any() else []\n",
    "    \n",
    "    return batch_sentA, batch_sentB, batch_seq_lenA, batch_seq_lenB, batch_label\n",
    "    \n",
    "    \n",
    "def accuracy(probability, label):\n",
    "    prediction = np.round(probability)\n",
    "    return np.mean(np.equal(prediction, label))\n",
    "\n",
    "\n",
    "def prediction_to_csv(test_id, prediction, filepath):\n",
    "    prediction_df = pd.DataFrame({'Id': test_id, 'Category': prediction})\n",
    "    prediction_df['Category'] = prediction_df['Category'].map({0: 'agreed', 1: 'disagreed', 2: 'unrelated'})\n",
    "    prediction_df.to_csv(filepath, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/train.csv'\n",
    "train_df = read_train_data(train_file)\n",
    "sentA, sentB, label = train_df['title1_zh'].values, train_df['title2_zh'].values, train_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df['label'].value_counts())\n",
    "train_df.hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_file = 'wordvector/zhwiki300-word2vec.txt'\n",
    "UNK, PAD = '<UNK>', '<PAD>'\n",
    "wordvector, word2index, vocab = load_wordvector(wordvec_file, UNK, PAD)\n",
    "print('emb_dim: {}'.format(len(wordvector[0])))\n",
    "print('vocab_size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentA, sentB = process_unknown(sentA, sentB, set(vocab), UNK)\n",
    "sentA, sentB = word_to_index(sentA, sentB, word2index)\n",
    "seq_lenA = np.array([len(sent) for sent in sentA])\n",
    "seq_lenB = np.array([len(sent) for sent in sentB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/sentA.npy', sentA)\n",
    "np.save('data/sentB.npy', sentB)\n",
    "np.save('data/seq_lenA.npy', seq_lenA)\n",
    "np.save('data/seq_lenB.npy', seq_lenB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encode Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(sparse=False)\n",
    "label = onehotencoder.fit_transform(label.reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/label.npy', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.nn import embedding_lookup, softmax\n",
    "from tensorflow.contrib.rnn import LSTMCell\n",
    "from tensorflow.nn import bidirectional_dynamic_rnn, softmax_cross_entropy_with_logits_v2\n",
    "from tqdm import tqdm_notebook\n",
    "import os, json\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanceDetector:\n",
    "    def __init__(self, config, emb_matrix):\n",
    "        tf.reset_default_graph()\n",
    "        self.config = config\n",
    "        self.num_class = config['num_class']\n",
    "        self.lstm_unit = config['lstm_unit']\n",
    "        self.emb_dim = config['emb_dim']\n",
    "        self.vocab_size = config['vocab_size']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.sentA = tf.placeholder(tf.int32, shape=(None, None))  # (batch_size, sent_len)\n",
    "        self.sentB = tf.placeholder(tf.int32, shape=(None, None))  # (batch_size, sent_len)\n",
    "        self.seq_lenA = tf.placeholder(tf.int32, shape=(None, ))   # (batch_size, )\n",
    "        self.seq_lenB = tf.placeholder(tf.float32, shape=(None, 1))   # (batch_size, 1)\n",
    "        self.stance = tf.placeholder(tf.int32, shape=(None, config['num_class']))  # (batach_size, num_class)\n",
    "        \n",
    "        emb_shape = (self.vocab_size, self.emb_dim)\n",
    "        self.emb_matrix = tf.get_variable(shape=emb_shape, \n",
    "                                    initializer=tf.constant_initializer(emb_matrix, dtype=tf.float32),\n",
    "                                    dtype=tf.float32,\n",
    "                                    trainable=config['emb_trainable'],\n",
    "                                    name='embedding_matrix')\n",
    "        \n",
    "        self.Wa = tf.get_variable('attenion_weight', (self.emb_dim * 2, 1)) \n",
    "        self.ba = tf.get_variable('attention_bias', (1, ))\n",
    "        self.Wo = tf.get_variable('output_weight', (self.lstm_unit * 2, self.num_class))\n",
    "        self.bo = tf.get_variable('output_bias', (self.num_class, ))        \n",
    "\n",
    "        \n",
    "    def embedding_layer(self, sequence):\n",
    "        return embedding_lookup(self.emb_matrix, sequence)\n",
    "    \n",
    "    \n",
    "    def BiLSTM(self, sequence, seq_len):\n",
    "        # sequence shape: (batch_size, sent_len, emb_dim)\n",
    "        # seq_len shape: (batch_size)\n",
    "        \n",
    "        cell_fw = LSTMCell(num_units=self.lstm_unit)\n",
    "        cell_bw = LSTMCell(num_units=self.lstm_unit)\n",
    "        \n",
    "        ((output_fw, output_bw), _) = bidirectional_dynamic_rnn(cell_fw, cell_bw, sequence, dtype=tf.float32, sequence_length=seq_len)\n",
    "        \n",
    "        # output_fw shape: (batch_size, sent_len, unit)\n",
    "        # output_bw shape: (batch_size, sent_len, unit)\n",
    "        context = tf.concat([output_fw, output_bw], axis=2)  # (batch_size, sent_len, unit * 2)\n",
    "        return context\n",
    "    \n",
    "    def attention_layer(self, sequence):\n",
    "        # sequence shape: (batch_size, sent_len, emb_dim * 2)\n",
    "        \n",
    "        def fn(sent):\n",
    "            # sent shape: (sent_len, emb_dim * 2)\n",
    "            return tf.matmul(sent, self.Wa) + self.ba  # (sent_len, 1)\n",
    "        \n",
    "        # iterate each batch sentence\n",
    "        return softmax(tf.map_fn(fn, sequence)) # (batch_size, sent_len, 1)\n",
    "\n",
    "    \n",
    "    def build(self):\n",
    "        \n",
    "        ##### Context Representation #####\n",
    "        self.x = self.embedding_layer(self.sentA)  # (batch_size, sent_lenA, emb_dim)\n",
    "        self.h = self.BiLSTM(self.x, self.seq_lenA)     # (batch_size, sent_lenA, lstm_unit * 2)\n",
    "        \n",
    "        ##### Target-augmented Embedding #####\n",
    "        sent_lenA = tf.shape(self.x)[1]\n",
    "        target_info = self.embedding_layer(self.sentB)  # (batch_size, sent_lenB, emb_dim)\n",
    "        \n",
    "        z = tf.reduce_sum(target_info, axis=1)          # (batch_size, emb_dim)\n",
    "        z = tf.divide(z, self.seq_lenB)                 # (batch_size, emb_dim)\n",
    "        z = tf.expand_dims(z, axis=1)                   # (batch_size, 1, emb_dim)\n",
    "        self.z = tf.tile(z, [1, sent_lenA, 1])               # (batch_size, sent_lenA, emb_dim)\n",
    "        self.e = tf.concat([self.x, self.z], axis=2)                   # (batch_size, sent_lenA, emb_dim * 2)\n",
    "        \n",
    "        ##### Target-specific Attention Extraction #####\n",
    "        self.a = self.attention_layer(self.e)  # (batch_size, sent_len, 1)        \n",
    "        \n",
    "        ##### Stance Classification #####\n",
    "        s = self.h * self.a  # (batch_size, sent_len, lstm_unit * 2)\n",
    "        self.s = tf.reduce_mean(s, axis=1)  # (batch_size, lsmt_unit * 2)\n",
    "        p = tf.matmul(self.s, self.Wo) + self.bo  # (batch_size, num_class)\n",
    "        self.output = softmax(p)\n",
    "        \n",
    "        ##### Cross Entropy #####\n",
    "        cross_entropy = softmax_cross_entropy_with_logits_v2(labels=self.stance, logits=p)  # (batch_size, )\n",
    "        self.loss = tf.reduce_mean(cross_entropy)  # ()\n",
    "        \n",
    "        ##### L2 Regularization #####\n",
    "        L2_lambda = 0.01\n",
    "        L2 = L2_lambda * tf.reduce_sum([tf.nn.l2_loss(tf_var) \n",
    "                                        for tf_var in tf.trainable_variables() \n",
    "                                        if not('bias' in tf_var.name)]) # or 'embedding' in tf_var.name)])\n",
    "        self.loss += L2\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    \n",
    "    def fit(self, train_data, val_data, epoch_size, batch_size, word2index, model_name):\n",
    "        def learn(data, epoch, mode):\n",
    "            tn = tqdm_notebook(total=len(data[0]))\n",
    "            nbatch, epoch_loss, epoch_acc = 0, 0, 0 \n",
    "            for sentA, sentB, seq_lenA, seq_lenB, label in next_batch(data, batch_size, word2index):\n",
    "                feed_dict = {\n",
    "                    self.sentA: sentA,\n",
    "                    self.sentB: sentB, \n",
    "                    self.seq_lenA: seq_lenA,\n",
    "                    self.seq_lenB: seq_lenB.reshape((-1, 1)),\n",
    "                    self.stance: label\n",
    "                }\n",
    "                if mode == 'train':\n",
    "                    fetches = [self.loss, self.output, self.optimizer, self.h, self.x]\n",
    "                    loss, output, _, h, x = self.sess.run(fetches, feed_dict)\n",
    "                    tn.set_description('Epoch: {}/{}'.format(epoch, epoch_size))\n",
    "                elif mode == 'validate':                    \n",
    "                    fetches = [self.loss, self.output]\n",
    "                    loss, output = self.sess.run(fetches, feed_dict)\n",
    "                \n",
    "                acc = accuracy(output, label)\n",
    "                tn.set_postfix(loss=loss, accuracy=acc, mode=mode)\n",
    "                tn.update(n=batch_size)\n",
    "            \n",
    "            return [loss, acc]\n",
    "                \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "        train_log, val_log = [], []\n",
    "        print('Train on {} samples, validate on {} samples'.format(len(train_data[0]), len(val_data[0])))\n",
    "        for epoch in range(1, epoch_size + 1):       \n",
    "            train_data = shuffle_data(train_data)\n",
    "            # train\n",
    "            train_log.append(learn(train_data, epoch, 'train'))\n",
    "\n",
    "            # validate\n",
    "            if len(val_data[0]) > 0:\n",
    "                val_log.append(learn(val_data, epoch, 'validate')) \n",
    "        \n",
    "        self.save(model_name, train_log, val_log)\n",
    "     \n",
    "    \n",
    "    def predict(self, data, word_to_index):\n",
    "        \n",
    "        y_empty = np.empty(0)\n",
    "        batch_size, i = 500, 0\n",
    "        tn = tqdm_notebook(total=len(data[0]))\n",
    "        prediction = np.empty((len(data[0]), 3))\n",
    "        for sentA, sentB, seq_lenA, seq_lenB, _ in next_batch(data, batch_size, word_to_index):\n",
    "            fetches = [self.output]\n",
    "            feed_dict = {\n",
    "                self.sentA: sentA,\n",
    "                self.sentB: sentB, \n",
    "                self.seq_lenA: seq_lenA,\n",
    "                self.seq_lenB: seq_lenB.reshape((-1, 1)),\n",
    "            }\n",
    "            output = self.sess.run(fetches, feed_dict)[0]\n",
    "            prediction[i * batch_size: i * batch_size + len(output)] = output\n",
    "            \n",
    "            tn.set_postfix(mode='predict')\n",
    "            tn.update(n=batch_size)\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        \n",
    "        return np.argmax(prediction, axis=1)\n",
    "    \n",
    "    \n",
    "    def save(self, model_name, train_log, val_log):\n",
    "        model_dir = 'models/{}'.format(model_name)\n",
    "        if not os.path.isdir(model_dir):\n",
    "            os.mkdir(model_dir)\n",
    "            os.mkdir('{}/result'.format(model_dir))\n",
    "        \n",
    "        # save model\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(self.sess, '{}/{}.ckpt'.format(model_dir, model_name))\n",
    "        \n",
    "        # save config\n",
    "        with open('{}/config.json'.format(model_dir), 'w', encoding='utf-8') as file:\n",
    "            json.dump(self.config, file)\n",
    "            \n",
    "        # save log\n",
    "        with open('{}/log'.format(model_dir), 'w', encoding='utf-8') as file:\n",
    "            for i in range(len(train_log)):\n",
    "                tlog = train_log[i]\n",
    "                vlog = val_log[i] if len(val_log) > 0 else []\n",
    "                log_str = 'Epoch {}: train_acc={}, train_loss={}'.format(i+1, tlog[0], tlog[1])\n",
    "                log_str += ', val_acc={}, val_loss={}'.format(vlog[0], vlog[1]) if vlog else ''\n",
    "                file.write(log_str + '\\n')\n",
    "            \n",
    "        print('Model was saved in {}'.format(save_path))\n",
    "    \n",
    "    \n",
    "    def restore(self, model_path):\n",
    "        saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "        saver.restore(self.sess, model_path)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_class': 3,\n",
    "    'lstm_unit': 200,\n",
    "    'emb_dim': len(wordvector[0]),\n",
    "    'vocab_size': len(vocab),\n",
    "    'learning_rate': 1e-2,\n",
    "    'wordvec': wordvec_file,\n",
    "    'emb_trainable': False\n",
    "}\n",
    "detector = StanceDetector(config, wordvector)\n",
    "detector.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentA, sentB, seq_lenA, seq_lenB, label = \\\n",
    "    np.load('data/sentA.npy'), np.load('data/sentB.npy'), np.load('data/seq_lenA.npy'), np.load('data/seq_lenB.npy'), np.load('data/label.npy')\n",
    "train_data, val_data = train_val_split(sentA, sentA, seq_lenA, seq_lenA, label, train_ratio=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = 3\n",
    "batch_size = 32\n",
    "model_name = 'model-3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.fit(train_data, val_data, epoch_size, batch_size, word2index, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = read_test_data('data/test.csv')\n",
    "test_id, test_sentA, test_sentB = test_df['id'].values, test_df['title1_zh'].values, test_df['title2_zh'].values\n",
    "\n",
    "test_sentA, test_sentB = process_unknown(test_sentA, test_sentB, set(vocab), UNK)\n",
    "test_sentA, test_sentB = word_to_index(test_sentA, test_sentB, word2index)\n",
    "test_seq_lenA = np.array([len(sent) for sent in test_sentA])\n",
    "test_seq_lenB = np.array([len(sent) for sent in test_sentB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/test_id.npy', test_id)\n",
    "np.save('data/test_sentA.npy', test_sentA)\n",
    "np.save('data/test_sentB.npy', test_sentB)\n",
    "np.save('data/test_seq_lenA.npy', test_seq_lenA)\n",
    "np.save('data/test_seq_lenB.npy', test_seq_lenB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'num_class': 3,\n",
    "    'lstm_unit': 200,\n",
    "    'emb_dim': len(wordvector[0]),\n",
    "    'vocab_size': len(vocab),\n",
    "    'learning_rate': 1e-2,\n",
    "    'wordvec': wordvec_file,\n",
    "    'emb_trainable': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stancedetector = StanceDetector(config, wordvector)\n",
    "stancedetector.build()\n",
    "stancedetector.restore('models/{}/{}.ckpt'.format(model_name, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id, test_sentA, test_sentB, test_seq_lenA, test_seq_lenB = \\\n",
    "    np.load('data/test_id.npy'), np.load('data/test_sentA.npy'), np.load('data/test_sentB.npy'), np.load('data/test_seq_lenA.npy'), np.load('data/test_seq_lenB.npy')\n",
    "test_data = [test_sentA, test_sentB, test_seq_lenA, test_seq_lenB, np.empty(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = stancedetector.predict(test_data, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = 'models/{}/result/result.csv'.format(model_name)\n",
    "prediction_to_csv(test_id, prediction, result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
